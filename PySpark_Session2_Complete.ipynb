{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e71ab2-d8ea-477f-b8de-a39c7bf4dd1c",
   "metadata": {},
   "source": [
    "So far we've used toy examples to introduce the RDD API along with a few of its Transformations and Actions. Now let's look at a more real-life example: let's wrangle a fairly big \"semi-structured\" file and turn it into something a Data Scientist would be ready to work with. In fact, let's ask a few Data Science-y questions of this data and use Spark itself to answer them while we are at it!\n",
    "\n",
    "In order to continue with this lesson, first download the required data files and put it in your data folder inside your working directory.\n",
    "\n",
    "This example file is a standard Apache web server log. It's the logs from a month's worth of requests to NASA's website, in the distant year of 1995, combined into one fairly big file to be more specific.\n",
    "\n",
    "This log contains the following information:\n",
    "\n",
    "* The IP Address or the DNS name performing a request\n",
    "* A time stamp of the form: \"dd/Mon/YYYY:hh:mm:ss Timezone\"\n",
    "* The request type (HTTP verb), the resource being requested and the Protocol used\n",
    "* The code returned by the server (200 OK, 400 Not Found etc...)\n",
    "* The Size of the resource being requested\n",
    "\n",
    "We will use the textFile method to read in this file. This, like the parallelize method, turns the data inside this file into an RDD. There are two important things you need to know about this method: \n",
    "\n",
    "* In a real-life Spark Cluster, the location of the file (the argument you will pass to textFile) must be visible/accessible to all nodes of the Cluster. In practice, a lot of the time this location will be a path on a Hadoop Distributed File System (HDFS), but this can be any Network File System, or a location mounted on all nodes, or Amazon S3... as long as it's visible and accessible on all nodes! \n",
    "\n",
    "* This method turns each line of the input file into an element in a Partition. So ,no matter what the format of the file is, when it gets turned into an RDD, each line (as delimited by a newline a.k.a. \"\\n\") becomes an element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6658ed1-af8d-4d81-8fb0-f9826e7e3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's import pyspark, initialize a Spark connection\n",
    "\n",
    "\n",
    "#Let's read NASA logs as a textfile into a variable and find out the type of variable we defined here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcfe16b-3641-45ba-92a2-c4b4c53217e4",
   "metadata": {},
   "source": [
    "The first step in any data problem is to look at the data to get a sense of what we are dealing with. A good practice is to find out how many elements we have to get a sense of what we are dealing with. The RDD API has the count method for that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c44f2-d7ba-4984-805d-3a6ab452aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use count() method to see how many elements (lines) are in the NASA logs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f79a7-a68f-4252-ac4e-872e4df55410",
   "metadata": {},
   "source": [
    "The RDD API has the take Action, that brings a number of elements (remember, an element here is a line of the original file) back to the Driver so we can see them. The important thing here is to be careful not to bring too many elements back to Driver and blow up its memory capacity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8333b5e-e6f4-4cf2-a307-a9dc6fd2a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use take() action to bring a number of elements from Cluster back to the Driver!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05ca7ba-3c1a-4128-8160-aac8dd22bb7f",
   "metadata": {},
   "source": [
    "Now that we can see what the data looks like, a reasonable first step seems to be to split the data on the \" \" (space) character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be64964-6234-4768-a29d-aedfb2bb3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data on space characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bfc5f5-0d20-4283-b413-e3cf7bce1d42",
   "metadata": {},
   "source": [
    "Next, for the sake of this example, let's say we are not interested in lines where there is data missing. In other words, we are only interested in lines that have all 10 elements. We will use the filter method to filter any lines that don't have all 10 elements out of our RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b148a1f-fb8d-42a7-8693-1575d4f77da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data and count the lines that have 10 exactly elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8b0c8-8490-47b5-9f63-66fcc463a921",
   "metadata": {},
   "source": [
    "\n",
    "This line of code in PySpark performs a series of operations to count the number of lines in the \n",
    "nasa_logs RDD that have exactly 10 words (or elements) separated by spaces.\n",
    "\n",
    "You might be asking yourself whether using the take method all the time to check if we are doing things right is the best practice... and the answer is no. Everytime you call it, you are computing a new RDD and thus having the Spark Cluster do work for you. In real-life you will rarely have a Cluster all for yourself, so you should expect your computations to get queued and competing for resources with other users. in this scenario, minimizing the amount of times you move things back and forth between the Driver and the Executors is a good idea.\n",
    "\n",
    "So in practice, one approach would be to use the RDD API method sample to extract a sample of your data to examine in the driver and figure out what you need to do before farming out computations to the cluster. The take method also works here, but getting a random sample (using sample() method) instead of the first N elements of your RDD is almost always a better plan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf0371-dc63-41d6-a7b1-970f47ca030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you know how much data 0.01% of your dataset is! \n",
    "#It might look like a small fraction, but in the Big Data world \n",
    "#even that might be too much for your local computer!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb60fd0-d533-42b2-baba-de95fdca16f6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Web server logs like this are called 'semi-structured' for a reason: we can be pretty sure that every line will be formatted the same way. This means every element in each of our Partitions looks pretty much the same after our first step. We can be confident that the same unwanted characters ended up inside the elements of all partitions of our RDD. So our next step takes care of removing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de336a1-8093-42f2-b43b-8088ee66d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning: remove the unwanted characters\n",
    "# The dictionary maps three unwanted characters ([, ], and \") to empty strings ('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dbc0e5-9a29-40d1-9312-09c2fac51c0a",
   "metadata": {},
   "source": [
    "In summary, this code cleaned the data by removing square brackets and double quotes from each element in lines that have exactly 10 elements.\n",
    "\n",
    "* The translate() method returns a string where some specified characters are replaced with the character described in a dictionary, or in a mapping table.\n",
    "* The str.maketrans(replacement_dict) part creates a translation table that maps the keys (characters) to their corresponding values (empty strings)\n",
    "* The element.translate(...) call uses this translation table to remove the characters specified in the replacement_dict keys from the string element.\n",
    "\n",
    "Ok, so now our RDD has the following elements: \n",
    "\n",
    "IP/NAME_OF_ORIGIN \n",
    "DATE/TIME, TIMEZONE\n",
    " REQUEST_METHOD\n",
    " RESOURCE_REQUESTED\n",
    " PROTOCOL\n",
    " STATUS_CODE, SIZE_OF_RESOURCE\n",
    "\n",
    "That looks pretty much like a CSV (or a Dataframe) a Data Scientist could work with! We aim to take advantage of our now-structured dataset and see if we can do a bit of Data Science using the RDD API directly. Let's find out where most requests to the NASA web server came from on our dataset. To do this, let's do a little bit of Map-Reduce.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c2e1e7-ed5f-4cd4-809d-4f906202bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take each line of our structured log and return a Key-Value Pair\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982a2fd3-2c32-467a-82e7-77a2d24e7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and return the five most frequent encounters \n",
    "# like the count program, we create a tuple containing the encounters and a count of 1 \n",
    "# representing its initial occurrence, then compute the total count.\n",
    "\n",
    "\n",
    "\n",
    "# The second map() transforms the DataFrame to have the count as the first element \n",
    "# for sorting purposes and the word as the second element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e5544-d166-4b46-abe3-7d88d1d80243",
   "metadata": {},
   "source": [
    "\n",
    "Exercise 3.1 - Word count in NASA log\n",
    "\n",
    "If we take the element containing NASA's website resource names and we replace the \"/\"s and \".\"s by \" \"s, we sort of get words. Write a word count program to find the top 5 most frequent words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df50e6dd-2aee-4ed5-8915-db3d195f329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step essentially should splits the resource name based on\n",
    "# / and . characters and treats them as word separators.\n",
    "# The result should be a new DataFrame containing these \"cleaned-up\" website resource names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2279c35-f5db-4c0b-bb3f-b30a9ff40d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step effectively should create a new DataFrame\n",
    "# where each row is a single word extracted from the website resource names.\n",
    "# Use flatMap() method to flattening the list of words into a single level.\n",
    "# Then use a map() transformation to create your count tuple\n",
    "# Finally use a reduceByKey action to calculate the total count for each word\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63e098-3ac9-428c-a15f-be208820325a",
   "metadata": {},
   "source": [
    "\n",
    "Reading a CSV file with Core Spark API (RDD API):\n",
    "\n",
    "The RDD API is very powerful, but on its own it has some serious limitations. Ironically, one of its biggest limitations is its usefulness on structured data... like CSV files.\n",
    "\n",
    "We had caught a glimpse of that on the NASA website example, but now let's look at a real-life CSV to illustrate this and introduce the Pandas on Spark API - a powerful API for which the RDD API can work as a beautiful complement.\n",
    "\n",
    "The file below contains data about all pieces owned/maintained by the Metropolitan Museum of Art in New York City. As we've seen before, the RDD API only allows us to load it as a plain text file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d14c57-da57-461f-ad1b-07be147fbdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c168f-f3e5-4e28-88ff-fae6f57fac7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea7155-cb76-44ff-a449-82145081a653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cdd59a-ec24-4099-a37d-2be7c5e478ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e058dc-d4ba-4c7f-9b60-bcd13ada33f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6760fc08-1bda-48a1-81c5-c93a944bfc0f",
   "metadata": {},
   "source": [
    "Spark Pandas API:\n",
    "\n",
    "The Spark Core RDD API is a powerful tool for operating on very large Data. However, the RDD API and its Functional Programming flavor are not for everyone. Most people dealing with heavy-duty data analytics problems are used to far more structured data types. Whether they're R users or Python users, data people love data that is in a tabular format - a Table in database or a DataFrame in R or Pandas. In most data analysis situations, it is important to be able to mimic some functionality and design choices from the Pandas package as one of the most powerful python packages to analyze data. To cater to this particular user base, Spark maintainers have introduced a new API in Spark v3: Pandas on Spark. As the name suggests, the idea behind this API is to reproduce the user experience from the Pandas package with as many of its methods and operators as possible, but on a very large scale distributed DataFrames. Note that as this API is actively being developed, you might encounter some errors with some functions. Usually, downgrading PySpark or Pandas version can fix those issues.\n",
    "\n",
    "To get started, first let's import the module pyspark.pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d50dff-80d4-4c17-a33f-33d6cdeda68e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import PySpark Pandas API\n",
    "#ignore the warning!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff3d838-248f-4082-a741-2fda4054330d",
   "metadata": {},
   "source": [
    "A handy way of using Pandas on Spark is by converting an actual Pandas DataFrame into a Pandas on Spark DataFrame. In this scenario, you would have a regular Pandas DataFrame, created without any calls to Spark that you wish to perform work on in a parallelized or even distributed fashion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe4534-5e55-4701-8c71-953fa792b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create a Pandas dataframe from a CSV file and then convert it to Pandas on Spark DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a588824-0c4d-44e0-9760-92d298566ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e249a-a1da-4579-9187-78619eae462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's create a Spark DataFrame from a Pandas DataFrame!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2a3924-aaab-4c2f-8396-66184c25215c",
   "metadata": {},
   "source": [
    "Now we have a parallelized or distributed DataFrame that looks and behaves just like a regular Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5781840-abb9-4dd7-9ff8-336a7b09a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at this DataFrame using some familiar methods: first head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90dd09e-2429-4206-b93a-58b95907bfc8",
   "metadata": {},
   "source": [
    "Next, we will go through a few examples of how to use Pandas on Spark DataFrame. Accessing columns and rows, as well as slicing a DataFrame works just like in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dffa16-c4ed-4802-bfc4-2ca02be88a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Access columns by name with two different syntaxes:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf780b88-ba9f-4c14-a0c7-fa6513af0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .iloc() method to access a row by index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e8912-e363-4a07-adc8-1ef355d68ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use conditionals to find subsets of a DataFrame that match a condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3399b1-a88a-4a20-918a-16139bce80c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081fd0da-5a73-44d9-a283-8d86583ff7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location measures\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425aa86-e2b3-4a1a-9d9e-989ccad88f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dispersion measures:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d71f7-8dd4-4ca8-8a67-0c35943b696f",
   "metadata": {},
   "source": [
    "Notes: \n",
    "\n",
    "* Parallelizing a DataFrame does not necessarily mean any arbitrary operation will run faster. In general, you can expect Pandas on Spark to outperform Pandas as the size of a DataFrame grows, even if you are running PySpark on a single node. That being said, you should always reason about scalability before choosing to parallelize work over multiple cores, or multiple nodes. See this article for more about scalability: https://docs.alliancecan.ca/wiki/Scalability\n",
    "\n",
    "* Pandas on Spark is not a 100% perfect clone of Pandas - some Pandas functionalities have not yet been implemented, some probably never will be, and Pandas on Spark have a few features that do not exist on Pandas. See the complete API reference for more details: https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b2a154-e016-4ec1-83a5-1ab717a4a2d0",
   "metadata": {},
   "source": [
    "Exercise 4.1 \n",
    "\n",
    "Use pandas on Spark API:\n",
    "\n",
    "* Create a parallel query that finds all rows with a weight value greater than 50 and hindfoot_length larger than 52, and then calculate the summary statistics of these rows.\n",
    "\n",
    "* Hint: You can use where() method to introduce two different conditions in your search and dropna() method to remove rows with missing values in weight or hindfoot_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0a777-e9ad-4536-a720-b22411307cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 4.1 Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f3511-cf14-41e0-99d4-ed21b885d36e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
